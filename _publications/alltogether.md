---
title: AllTogether: Investigating the Efficacy of Spliced Prompt for Web Navigation using Large Language Models
collection: publications
permalink: /publication/alltogether
excerpt: 'Large Language Models (LLMs) have emerged as promising agents for web navigation tasks, interpreting objectives and interacting with web pages. However, the efficiency of spliced prompts for such tasks remains underexplored. We introduces AllTogether, a standardized prompt template that enhances task context representation, thereby improving LLMs' performance in HTML-based web navigation. We evaluate the efficacy of this approach through prompt learning and instruction finetuning based on open-source Llama-2 and API-accessible GPT models. Our results reveal that models like GPT-4 outperform smaller models in web navigation tasks. Additionally, we find that the length of HTML snippet and history trajectory significantly influence performance, and prior step-by-step instructions prove less effective than real-time environmental feedback. Overall, we believe our work provides valuable insights for future research in LLM-driven web agents.'
date: 2023-10-20
venue: 'arxiv preprint'
paperurl: 'https://arxiv.org/pdf/2310.18331v1.pdf'
citation: 'Hu, W., Jiang, X., Liu, J., Yang, Y., & Tian, H. (2023). Meta-DM: Applications of Diffusion Models on Few-Shot Learning. arXiv preprint arXiv:2305.08092.'
---

Large Language Models (LLMs) have emerged as promising agents for web navigation tasks, interpreting objectives and interacting with web pages. However, the efficiency of spliced prompts for such tasks remains underexplored. We introduces AllTogether, a standardized prompt template that enhances task context representation, thereby improving LLMs' performance in HTML-based web navigation. We evaluate the efficacy of this approach through prompt learning and instruction finetuning based on open-source Llama-2 and API-accessible GPT models. Our results reveal that models like GPT-4 outperform smaller models in web navigation tasks. Additionally, we find that the length of HTML snippet and history trajectory significantly influence performance, and prior step-by-step instructions prove less effective than real-time environmental feedback. Overall, we believe our work provides valuable insights for future research in LLM-driven web agents.

#### ðŸ“Œ This paper is still under review with COLING 2024. You can check the arxiv preprint version.

[Download paper here](https://arxiv.org/pdf/2310.18331v1.pdf)
