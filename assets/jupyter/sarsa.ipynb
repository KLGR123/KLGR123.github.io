{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ed8d379",
   "metadata": {},
   "source": [
    "使用查找表的强化学习方法称为表格型方法（tabular method），如蒙特卡洛、Q 学习和 Sarsa。\n",
    "- 蒙特卡洛方法是基于采样的方法，给定策略 π，我们让智能体与环境进行交互，可以得到很多轨迹；每个轨迹都有对应的回报；求出所有轨迹的回报的平均值，就可以知道某一个策略对应状态的价值；\n",
    "- 把经验均值（empirical mean）转换成增量均值（incremental mean）：\n",
    "- $$\\begin{aligned}\\mu_{t} & =\\frac{1}{t} \\sum_{j=1}^{t} x_{j} \\\\& =\\frac{1}{t}\\left(x_{t}+\\sum_{j=1}^{t-1} x_{j}\\right) \\\\& =\\frac{1}{t}\\left(x_{t}+(t-1) \\mu_{t-1}\\right) \\\\& =\\frac{1}{t}\\left(x_{t}+t \\mu_{t-1}-\\mu_{t-1}\\right) \\\\& =\\mu_{t-1}+\\frac{1}{t}\\left(x_{t}-\\mu_{t-1}\\right)\\end{aligned}$$\n",
    "- 把蒙特卡洛方法更新的方法写成增量式蒙特卡洛（incremental MC）方法：\n",
    "- $$V\\left(s_{t}\\right) \\leftarrow V\\left(s_{t}\\right)+\\alpha\\left(G_{t}-V\\left(s_{t}\\right)\\right)$$\n",
    "- 蒙特卡洛方法相比动态规划方法是有一些优势的：首先，蒙特卡洛方法适用于环境未知的情况，而动态规划是有模型的方法；蒙特卡洛方法只需要更新一条轨迹的状态，而动态规划方法需要更新所有的状态；状态数量很多的时候（比如100万个、200万个），我们使用动态规划方法进行迭代，速度是非常慢的；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe29bb5",
   "metadata": {},
   "source": [
    "时序差分是介于蒙特卡洛和动态规划之间的方法，它是免模型的，不需要马尔可夫决策过程的转移矩阵和奖励函数。时序差分方法可以从不完整的回合中学习，并且结合了自举的思想。\n",
    "- 最简单的算法是一步时序差分（one-step TD），即TD(0)；\n",
    "- 每往前走一步，就做一步自举，用得到的估计回报更新上一时刻的价值；\n",
    "- $V\\left(s_{t}\\right) \\leftarrow V\\left(s_{t}\\right)+\\alpha\\left(r_{t+1}+\\gamma V\\left(s_{t+1}\\right)-V\\left(s_{t}\\right)\\right)$\n",
    "- 时序差分方法只执行一步，状态的值就更新；蒙特卡洛方法全部执行完之后，到了终止状态之后，再更新它的值；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a3818f",
   "metadata": {},
   "source": [
    "在第二章的习题中，提到了第 n 步的价值函数更新公式：\n",
    "- - $Q(S, A) \\leftarrow Q(S, A)+\\alpha\\left[\\sum_{i=1}^{n} \\gamma^{i-1} r_{t+i}+\\gamma^{n} \\max _{a} Q\\left(S^{\\prime}, a\\right)-Q(S, A)\\right]$\n",
    "- n 越大，方差越大，期望偏差越小；与蒙特卡洛方法相比，时序差分方法有如下几个优势：低方差，能够在线学习，能够从不完整的序列中学习；\n",
    "- 这里并非是对单一状态更新，而是所有状态；这也就是所谓动态规划算法和时序差分算法的区别；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e6aecb",
   "metadata": {},
   "source": [
    "- 时序差分方法可以在线学习（online learning），每走一步就可以更新，效率高；蒙特卡洛方法必须等游戏结束时才可以学习；\n",
    "- 时序差分方法可以从不完整序列上进行学习；蒙特卡洛方法只能从完整的序列上进行学习；因为必须有全部的奖励，没有任何估计部分；\n",
    "- 时序差分方法可以在连续的环境下（没有终止）进行学习；蒙特卡洛方法只能在有终止的情况下学习；\n",
    "- 时序差分方法利用了马尔可夫性质，在马尔可夫环境下有更高的学习效率；蒙特卡洛方法不需要假设环境具有马尔可夫性质，利用采样的价，值来估计某个状态的价值在不是马尔可夫的环境下更加有效；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb81c3e",
   "metadata": {},
   "source": [
    "再次整理一下思路：\n",
    "- 动态规划方法直接计算期望，它把所有相关的状态都进行加和，即 $$V\\left(s_{t}\\right) \\leftarrow \\mathbb{E}_{\\pi}\\left[r_{t+1}+\\gamma V\\left(s_{t+1}\\right)\\right]$$\n",
    "- 蒙特卡洛方法在当前状态下，采取一条支路，在这条路径上进行更新，更新这条路径上的所有状态，即 $$V\\left(s_{t}\\right) \\leftarrow V\\left(s_{t}\\right)+\\alpha\\left(G_{t}-V\\left(s_{t}\\right)\\right)$$\n",
    "- 时序差分从当前状态开始，往前走了一步，关注的是非常局部的步骤，即 $$\\mathrm{TD}(0): V\\left(s_{t}\\right) \\leftarrow V\\left(s_{t}\\right)+\\alpha\\left(r_{t+1}+\\gamma V\\left(s_{t+1}\\right)-V\\left(s_{t}\\right)\\right)$$\n",
    "\n",
    "回到策略迭代。当我们不知道奖励函数和状态转移时，如何进行策略的优化？我们引入了广义的策略迭代的方法。我们对策略评估部分进行修改，使用蒙特卡洛的方法代替动态规划的方法估计 **Q 函数**，便有基于探索性开始的蒙特卡洛方法；即把上式中的 V 估计换成 Q 估计（蒙特卡洛或时序差分），得到估计后从中采样出 $\\pi$ 并循环往复。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606043d4",
   "metadata": {},
   "source": [
    "Sarsa 所做出的改变很简单，它将原本时序差分方法更新 V 的过程，变成了更新 Q，即 $$Q\\left(s_{t}, a_{t}\\right) \\leftarrow Q\\left(s_{t}, a_{t}\\right)+\\alpha\\left[r_{t+1}+\\gamma Q\\left(s_{t+1}, a_{t+1}\\right)-Q\\left(s_{t}, a_{t}\\right)\\right]$$\n",
    "Sarsa 直接估计 Q 表格，得到 Q 表格后，就可以更新策略；也是 n = 1 的情况，n = 无穷时仍旧是 MC 方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342fba23",
   "metadata": {},
   "source": [
    "Sarsa 是一种同策略（on-policy）算法；同策略在学习的过程中，只存在一种策略，它用一种策略去做动作的选取，也用一种策略去做优化；Q学习是一种异策略（off-policy）算法；目标策略（target policy）和行为策略（behavior policy）。\n",
    "\n",
    "$$Q\\left(s_{t}, a_{t}\\right) \\leftarrow Q\\left(s_{t}, a_{t}\\right)+\\alpha\\left[r_{t+1}+\\gamma \\max _{a} Q\\left(s_{t+1}, a\\right)-Q\\left(s_{t}, a_{t}\\right)\\right]$$\n",
    "\n",
    "异策略的结构性优点：\n",
    "- 可以利用探索策略来学到最佳的策略，学习效率高；\n",
    "- 可以学习其他智能体的动作，进行模仿学习，学习人或者其他智能体产生的轨迹；\n",
    "- 可以重用旧的策略产生的轨迹，节省资源（探索过程需要很多计算资源）；\n",
    "\n",
    "Sarsa 和 Q 学习的更新公式是一样的，当然 Q 学习在学习的时候不需要传入下一个动作；区别只在目标计算的部分，Q 学习用了贪心策略而不是 ε-贪心策略，因此属于异策略。\n",
    "\n",
    "Q 学习并不需要知道我们实际上选择哪一个动作 ，它默认下一个动作就是 Q 值最大的那个动作。Q 学习知道实际上行为策略可能会有 0.1 的概率选择别的动作，但 Q 学习并不担心受到探索的影响，它默认按照最佳的策略去优化目标策略，所以它可以更大胆地去寻找最优的路径，它表现得比 Sarsa 大胆得多。\n",
    "\n",
    "**FrozenLake-v1** 是一个经典的强化学习环境：\n",
    "- 4x4 的网格世界\n",
    "- 智能体从起点(S)出发，目标是到达终点(G)\n",
    "- 冰面很滑，智能体可能会滑到其他方向\n",
    "- 掉入冰洞(H)会失败\n",
    "- 只有到达终点才能获得奖励(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78b8abda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "env.observation_space.n, env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ddb143e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "Q_table = np.zeros((16, 4), dtype=np.float64)\n",
    "state = env.reset()\n",
    "state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43de8237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "if random.random() < 0.1:\n",
    "    action = random.randint(0, 3)\n",
    "else:\n",
    "    action = np.argmax(Q_table[state[0]])\n",
    "\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08872f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_state, reward, done, info, _ = env.step(action)\n",
    "reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4426ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_value = Q_table[next_state, action]\n",
    "next_max = np.max(Q_table[next_state]) if not done else 0\n",
    "new_value = 0.8 * old_value + 0.2 * (reward + 0.9 * next_max)\n",
    "\n",
    "Q_table[0, 0] = new_value\n",
    "Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a3007e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
