{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81d71137",
   "metadata": {},
   "source": [
    "如下我们要验证 vLLM 的 continuous batching 过程，以及背后的一些机理和有趣的实验现象。\n",
    "\n",
    "请先运行 vllm_server.py 启动 vllm 服务，将如下脚本粘贴到外部 .py 文件中运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa3ccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.entrypoints.openai.api_server import run_server\n",
    "import asyncio\n",
    "\n",
    "MODEL_PATH = \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtsearch-assistant/ai-search/deepsearch_files/LLMbasemodels/huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507\"\n",
    "\n",
    "# vLLM engine configuration\n",
    "extra_kwargs = {\n",
    "    \"tensor_parallel_size\": 1,\n",
    "    \"gpu_memory_utilization\": 0.75,\n",
    "    \"max_model_len\": 4096,\n",
    "}\n",
    "\n",
    "# Default generation parameters\n",
    "generation_kwargs = {\n",
    "    \"temperature\": 0.3,\n",
    "    \"max_tokens\": 512,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    # Start vLLM OpenAI-compatible API server\n",
    "    cmd = [\n",
    "        \"vllm\", \"serve\",\n",
    "        MODEL_PATH,\n",
    "        \"--tensor-parallel-size\", str(extra_kwargs[\"tensor_parallel_size\"]),\n",
    "        \"--gpu-memory-utilization\", str(extra_kwargs[\"gpu_memory_utilization\"]),\n",
    "        \"--max-model-len\", str(extra_kwargs[\"max_model_len\"]),\n",
    "        \"--host\", \"0.0.0.0\",\n",
    "        \"--port\", \"8000\",\n",
    "    ]\n",
    "    \n",
    "    print(f\"Starting vLLM server with command:\")\n",
    "    print(\" \".join(cmd))\n",
    "    print(f\"\\nServer will be available at: http://0.0.0.0:8000\")\n",
    "    print(f\"API documentation: http://0.0.0.0:8000/docs\")\n",
    "    \n",
    "    subprocess.run(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69a29d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm functioning well, thank you for asking! 😊 I'm here and ready to help you with whatever you need. How can I assist you today?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"\",\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtsearch-assistant/ai-search/deepsearch_files/LLMbasemodels/huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n",
    "    temperature=0.3,\n",
    "    max_tokens=512,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd754f0",
   "metadata": {},
   "source": [
    "接下来我们验证 vLLM 的 continuous batching。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89b83eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts with different lengths\n",
    "test_prompts = [\n",
    "    \"今天天气怎么样？\",\n",
    "    \"请用三句话介绍一下你最喜欢的一本书。\",\n",
    "    \"写一首关于秋天的七言绝句。\",\n",
    "    \"解释量子计算和经典计算的区别，并举例说明。\",\n",
    "    \"推荐三个适合周末游玩的城市景点。\",\n",
    "    \"什么是区块链技术？请简要说明其工作原理。\",\n",
    "    \"用 Python 实现一个计算斐波那契数列的函数。\",\n",
    "    \"描述一下你理想中的假期是什么样的。\",\n",
    "    \"如何制作一杯美味的咖啡？\",\n",
    "    \"请解释一下相对论的基本概念。\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a5bea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cb2524",
   "metadata": {},
   "source": [
    "首先写一个并发发送多个请求的 demo，使得 vLLM 在后台确实实现了动态批处理。基于 asyncio 实现并发任务 gather。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb2f13da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "\n",
    "async def generate_responses(prompt) -> tuple[str, float]:\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtsearch-assistant/ai-search/deepsearch_files/LLMbasemodels/huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3,\n",
    "            max_tokens=512,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        length = len(response.choices[0].message.content)\n",
    "        end_time = time.time()\n",
    "        latency = end_time - start_time\n",
    "        return response.choices[0].message.content, latency, length\n",
    "    except Exception as e:\n",
    "        return None, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc3a5153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total latency: 9.954436302185059 seconds\n",
      "Prompt: 今天天气怎么样？\n",
      "Response: '我无法获取实时天气信息。建议你查看当地的天气预报，比如通过天气App、搜索引擎搜索“你所在城市+天气'\n",
      "Length: 89\n",
      "Latency: 1.3015964031219482 seconds\n",
      "\n",
      "Prompt: 请用三句话介绍一下你最喜欢的一本书。\n",
      "Response: '我最喜欢的一本书是《小王子》。它以简洁而诗意的语言，讲述了一个来自外星的小王子探索宇宙、寻找爱与意义'\n",
      "Length: 97\n",
      "Latency: 1.4555788040161133 seconds\n",
      "\n",
      "Prompt: 写一首关于秋天的七言绝句。\n",
      "Response: '金风拂叶舞斜阳，  \\n霜染层林映晚光。  \\n雁阵横天书字去，  \\n寒江独钓一蓑霜。'\n",
      "Length: 41\n",
      "Latency: 0.871971845626831 seconds\n",
      "\n",
      "Prompt: 解释量子计算和经典计算的区别，并举例说明。\n",
      "Response: '量子计算与经典计算是两种截然不同的计算范式，它们在基本原理、信息表示方式和处理能力上存在根本区别。以'\n",
      "Length: 923\n",
      "Latency: 9.950427293777466 seconds\n",
      "\n",
      "Prompt: 推荐三个适合周末游玩的城市景点。\n",
      "Response: '当然可以！以下是三个适合周末游玩的城市景点推荐，涵盖自然风光、人文历史与休闲体验，适合不同兴趣的游客'\n",
      "Length: 743\n",
      "Latency: 9.344392776489258 seconds\n",
      "\n",
      "Prompt: 什么是区块链技术？请简要说明其工作原理。\n",
      "Response: '区块链技术是一种分布式账本技术，它通过去中心化、加密和共识机制，确保数据的安全、透明和不可篡改。\\n\\n'\n",
      "Length: 525\n",
      "Latency: 6.176791429519653 seconds\n",
      "\n",
      "Prompt: 用 Python 实现一个计算斐波那契数列的函数。\n",
      "Response: '以下是几种用 Python 实现斐波那契数列的函数，从简单到高效：\\n\\n## 方法1：递归实现（简单但'\n",
      "Length: 1216\n",
      "Latency: 9.948890924453735 seconds\n",
      "\n",
      "Prompt: 描述一下你理想中的假期是什么样的。\n",
      "Response: '我理想中的假期，是一场与自然和内心深度对话的宁静旅程，没有匆忙的行程，也没有被计划填满的焦虑。它应该'\n",
      "Length: 525\n",
      "Latency: 7.157436847686768 seconds\n",
      "\n",
      "Prompt: 如何制作一杯美味的咖啡？\n",
      "Response: '制作一杯美味的咖啡，关键在于选择优质的咖啡豆、合适的研磨度、恰当的冲泡方法以及对水温与比例的把控。以'\n",
      "Length: 711\n",
      "Latency: 9.946262121200562 seconds\n",
      "\n",
      "Prompt: 请解释一下相对论的基本概念。\n",
      "Response: '相对论是现代物理学的基石之一，由阿尔伯特·爱因斯坦在20世纪初提出，主要分为两个部分：**狭义相对论'\n",
      "Length: 836\n",
      "Latency: 9.947412014007568 seconds\n",
      "\n",
      "Is latency_all the biggest? True\n"
     ]
    }
   ],
   "source": [
    "async def continuous_batching():\n",
    "    tasks = [generate_responses(prompt) for prompt in test_prompts]\n",
    "    start_time = time.time()\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    end_time = time.time()\n",
    "    latency_all = end_time - start_time\n",
    "    return results, latency_all\n",
    "\n",
    "async def main():\n",
    "    results, latency_all = await continuous_batching()\n",
    "    print(f\"Total latency: {latency_all} seconds\")\n",
    "    is_bigger = True\n",
    "    for prompt, (response, latency, length) in zip(test_prompts, results):\n",
    "        print(f\"Prompt: {prompt}\\nResponse: {repr(response[:50])}\\nLength: {length}\\nLatency: {latency} seconds\\n\")\n",
    "        if latency > latency_all:\n",
    "            is_bigger = False\n",
    "    print(f\"Is latency_all the biggest? {is_bigger}\")\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2056798",
   "metadata": {},
   "source": [
    "然后写一个 sequential 的串行场景，模仿无并发的情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fbe279b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 今天天气怎么样？\n",
      "Response: '我无法获取实时天气信息。建议您查看当地的天气预报应用或网站，如中国气象局官网、天气通、墨迹天气等，以'\n",
      "Length: 60\n",
      "Latency: 0.35826635360717773 seconds\n",
      "\n",
      "Prompt: 请用三句话介绍一下你最喜欢的一本书。\n",
      "Response: '我最喜欢的一本书是《小王子》。它以简洁而诗意的语言，讲述了一位来自外星的小王子在宇宙中旅行、探索爱与'\n",
      "Length: 97\n",
      "Latency: 0.537745475769043 seconds\n",
      "\n",
      "Prompt: 写一首关于秋天的七言绝句。\n",
      "Response: '金风拂叶舞斜阳，  \\n霜染层林映晚光。  \\n雁字横天书秋意，  \\n一江寒水送归航。'\n",
      "Length: 41\n",
      "Latency: 0.300952672958374 seconds\n",
      "\n",
      "Prompt: 解释量子计算和经典计算的区别，并举例说明。\n",
      "Response: '量子计算与经典计算是两种截然不同的信息处理范式，它们在基本原理、计算方式和应用潜力上存在根本区别。以'\n",
      "Length: 854\n",
      "Latency: 4.184512376785278 seconds\n",
      "\n",
      "Prompt: 推荐三个适合周末游玩的城市景点。\n",
      "Response: '当然可以！以下是三个适合周末游玩的城市景点推荐，涵盖自然风光、人文历史与休闲娱乐，适合不同兴趣的游客'\n",
      "Length: 762\n",
      "Latency: 4.192795753479004 seconds\n",
      "\n",
      "Prompt: 什么是区块链技术？请简要说明其工作原理。\n",
      "Response: '区块链技术是一种分布式账本技术，它通过去中心化、加密和共识机制，确保数据的安全、透明和不可篡改。\\n\\n'\n",
      "Length: 507\n",
      "Latency: 2.5272810459136963 seconds\n",
      "\n",
      "Prompt: 用 Python 实现一个计算斐波那契数列的函数。\n",
      "Response: '以下是几种用 Python 实现计算斐波那契数列的函数，从简单到高效：\\n\\n## 方法1：递归实现（简'\n",
      "Length: 1208\n",
      "Latency: 4.185424089431763 seconds\n",
      "\n",
      "Prompt: 描述一下你理想中的假期是什么样的。\n",
      "Response: '我理想中的假期，是一场与自然和内心深度对话的宁静之旅，没有匆忙的行程，没有打卡的压力，只有自由与放松'\n",
      "Length: 599\n",
      "Latency: 3.3914992809295654 seconds\n",
      "\n",
      "Prompt: 如何制作一杯美味的咖啡？\n",
      "Response: '制作一杯美味的咖啡，关键在于选材、研磨、水质和冲泡方法的配合。以下是使用手冲咖啡（推荐新手入门）的详'\n",
      "Length: 695\n",
      "Latency: 4.192051410675049 seconds\n",
      "\n",
      "Prompt: 请解释一下相对论的基本概念。\n",
      "Response: '相对论是现代物理学的两大支柱之一（另一个是量子力学），由阿尔伯特·爱因斯坦在20世纪初提出。它主要分'\n",
      "Length: 826\n",
      "Latency: 4.1914451122283936 seconds\n",
      "\n",
      "Total latency: 28.062873363494873 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for prompt in test_prompts:\n",
    "    response, latency, length = await generate_responses(prompt)\n",
    "    print(f\"Prompt: {prompt}\\nResponse: {repr(response[:50])}\\nLength: {length}\\nLatency: {latency} seconds\\n\")\n",
    "end_time = time.time()\n",
    "latency_all = end_time - start_time\n",
    "print(f\"Total latency: {latency_all} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234eadc1",
   "metadata": {},
   "source": [
    "如上，我们确实可以看到不同的 prompts 完成的时间各自不同，有的快有的慢，总体时间是 9.8 秒左右；不过，我们现在要比较的是它和（1）完全不 batching，即顺序 sequential 发送和处理（2）普通 batching，拿一批样本，padding 到同一长度后一次 forward 结束，弊端是短请求被长请求拖死的情况，类似基于 transformers 实现。\n",
    "\n",
    "我们需要观测到**确实**发生了 continuous batching，最有力的证据应该是不同 query 的处理顺序标记；而现在的情况也有可能是 vLLM 底层也只是退化地完成了普通 batching，毕竟当前的 batch 大小只有 10。根据如上结果可以看到，sequential 用了 28 秒左右，接近所有 test query 推理的时间总和。然而，我们也发现，相较于 asyncio 并发的情况，sequential 每条数据的推理时间几乎减少了一倍之多！这背后的原因是什么？目前的猜测大致是，由于串行使得 QPS 都用于当前 query 的推理，故而加速了？但具体的机制尚不清楚，毕竟如果是 batch 并行的，应该是不会牺牲单个 query 的速度的？\n",
    "\n",
    "让我们继续探究。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d9425f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safety",
   "language": "python",
   "name": "safety"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
