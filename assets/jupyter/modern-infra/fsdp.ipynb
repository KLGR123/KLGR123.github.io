{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337058af",
   "metadata": {},
   "source": [
    "**为什么 FSDP 比 DDP 好**\n",
    "\n",
    "DDP 是 每个GPU 上放一份完整的模型副本，前向和反向时都在本地算，最后用 all-reduce 来同步梯度。通信少，自然训练速度高，但更容易 OOM。以 https://huggingface.co/blog/zh/pytorch-fsdp 中提到的，在使用 GPT-2 XL (1.5B) 时，即使 batch size 为 1，DDP 也会失败并出现 OOM 错误（2 张 24GB 英伟达 Titan RTX GPU）；而 FSDP 可以支持以更大的 batch size 训练 GPT-2 Large 模型，还可以使用较大的 batch size 训练 DDP 训练不了的 GPT-2 XL 模型。\n",
    "\n",
    "FSDP (FullyShardedDataParallel) 之所以叫 Fully，就是因为它把模型参数、梯度、优化器状态都 shard（切分） 到不同 GPU 上，每张卡只保存自己负责的一部分；前向需要时再 all-gather 拼回参数；反向后再 reduce-scatter 梯度，最后释放显存。显存占用大大降低（近似 1/N，如果有 N 张卡）。HuggingFace 的 Accelerate 和 DeepSpeed（类似 Zero Stage 3）都在用。但因为有通信，训练速度可能不如 DDP，且代码配置更复杂，对 checkpointing、activation checkpoint 配合要求高。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97946f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model = nn.Linear(20, 2).to(accelerator.device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, decoupled_weight_decay=True, weight_decay=1e-2)\n",
    "dataloader = DataLoader(torch.randn(1000, 20).to(accelerator.device), batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a26f2861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AcceleratedOptimizer (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    decoupled_weight_decay: True\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, optimizer, dataloader = accelerator.prepare(model, optimizer, dataloader)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "263c9c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss: 0.21575644612312317\n",
      "step 1 loss: -1.9314888715744019\n",
      "step 2 loss: 1.2854886054992676\n",
      "step 3 loss: -1.0365784168243408\n",
      "step 4 loss: -1.563402533531189\n",
      "step 5 loss: -0.46200060844421387\n",
      "step 6 loss: -0.8780922293663025\n",
      "step 7 loss: -1.7737677097320557\n",
      "step 8 loss: -0.6850461363792419\n",
      "step 9 loss: 1.2737587690353394\n",
      "Training time: 0.0332 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(batch)\n",
    "    loss = 0.1 * outputs.sum()\n",
    "    print(f\"step {i} loss: {loss.item()}\")\n",
    "    accelerator.backward(loss)\n",
    "    optimizer.step()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "105afc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss: 3.0945417881011963\n",
      "step 1 loss: 2.743745803833008\n",
      "step 2 loss: 2.1409804821014404\n",
      "step 3 loss: 1.7734102010726929\n",
      "step 4 loss: 1.342269778251648\n",
      "step 5 loss: 1.5121549367904663\n",
      "step 6 loss: 2.2957770824432373\n",
      "step 7 loss: 3.3115932941436768\n",
      "step 8 loss: 2.401805877685547\n",
      "step 9 loss: 1.9694957733154297\n",
      "Training time: 0.0092 seconds\n"
     ]
    }
   ],
   "source": [
    "model_vanilla = nn.Linear(20, 2).to(\"cuda\")\n",
    "optimizer_vanilla = optim.Adam(model_vanilla.parameters(), lr=0.001, weight_decay=1e-2)\n",
    "dataloader_vanilla = DataLoader(torch.randn(1000, 20).to(\"cuda\"), batch_size=100, shuffle=True)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for i, batch in enumerate(dataloader_vanilla):\n",
    "    optimizer_vanilla.zero_grad()\n",
    "    outputs = model_vanilla(batch)\n",
    "    loss = 0.1 * outputs.sum()\n",
    "    print(f\"step {i} loss: {loss.item()}\")\n",
    "    loss.backward()\n",
    "    optimizer_vanilla.step()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e198265",
   "metadata": {},
   "source": [
    "具体而言，以 GPT-2 为例。假设我们有一个 2 层的 GPT-2（极简化）\n",
    "- 层 1：Embedding (30k vocab × 768 dim ≈ 23M 参数)\n",
    "- 层 2：Transformer Block (Attention + MLP ≈ 40M 参数)\n",
    "- 总参数 ≈ 63M\n",
    "\n",
    "如果我们用 4 张 GPU 来训练。\n",
    "\n",
    "**DDP**\n",
    "- 每张卡 完整的 63M 参数；\n",
    "- 每张卡算自己 batch 的 loss & backward；\n",
    "- 梯度通过 all-reduce 聚合，保证 4 张卡的参数保持一致；\n",
    "- 显存开销：参数 + 梯度 + 优化器状态（比如 Adam 需要额外存 m, v 两个同大小张量）；\n",
    "- 所以单卡显存大约是 3 × 63M；\n",
    "\n",
    "**显存大小的实际计算**\n",
    "- 注意，4 倍参数大小是理论最大开销；\n",
    "    - 在 PyTorch 里，梯度通常是和参数共享一块内存空间（或至少是相同大小的一份 buffer）；\n",
    "    - 优化器更新时是 in-place，因此不会额外再复制一份 grad；\n",
    "    - param (1x) + grad (1x 但复用 buffer) + m (1x) + v (1x) = 3 倍左右；\n",
    "\n",
    "- PyTorch 里 param 和 grad 的存储关系\n",
    "    - param：模型权重，比如存成 torch.float32 或 torch.bfloat16；\n",
    "    - grad：是 param 的 .grad，它的 dtype 和 param 一样；即如果参数是 bf16，反向计算出的梯度也是 bf16；\n",
    "    - 内存上，PyTorch 的 autograd 通常会分配一块 buffer，大小等于 param 的大小，用来存梯度；\n",
    "    - 这个 buffer 可以是独立开辟的，也可能和 param 共用 allocator 的 memory pool，但不会直接 alias 到 param 的存储空间（否则 param 会被覆盖）；\n",
    "    - 复用指的是显存管理策略（memory pool），而不是字节级别共用；\n",
    "    - param.dtype == grad.dtype，这是 PyTorch 的硬性约定，不会出现 param=bf16 而 grad=int32 这种情况；\n",
    "\n",
    "- 如果有混合精度 (fp32 / bf16 / fp16 / 量化)\n",
    "    - 参数和梯度保持同 dtype\n",
    "        - param 用 bf16，grad 就是 bf16；\n",
    "        - AdamW 更新时，需要把梯度 cast 到 FP32 master param 上；如果直接用 bf16 梯度去更新，数值范围太小，很容易 underflow / overflow；\n",
    "    - 参数低精度 + master 参数高精度\n",
    "        - param 存 bf16 / fp16（节省显存，forward / backward 加速）；\n",
    "        - 同时维护一份 FP32 的 master param（做优化器更新）；\n",
    "        - grad 会先 cast 到 FP32 再做更新；\n",
    "        - NVIDIA Apex、DeepSpeed ZeRO、FSDP mixed precision 都是这样做的；\n",
    "    - 量化（int8 / int4）模型\n",
    "        - 推理时，param 可能以 int8 存储，但计算时需要 dequantize 回到 fp16 / bf16 / fp32；\n",
    "        - 训练时几乎不会直接把 param 存 int8 的；\n",
    "    - 优化器状态（m, v）一定是 FP32\n",
    "        - 回顾一下 Adam 公式\n",
    "        $$\\begin{array}{l}m_{t}=\\beta_{1} m_{t-1}+\\left(1-\\beta_{1}\\right) g_{t} \\\\v_{t}=\\beta_{2} v_{t-1}+\\left(1-\\beta_{2}\\right) g_{t}^{2} \\\\\\theta_{t+1}=\\theta_{t}-\\eta \\frac{m_{t}}{\\sqrt{v_{t}}+\\epsilon}\\end{array}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4c4aa7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n",
      "torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.bfloat16, torch.bfloat16)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# param in bf16\n",
    "param = torch.randn(1000, dtype=torch.bfloat16, device=\"cuda\", requires_grad=True)\n",
    "\n",
    "# grad is bf16 too\n",
    "loss = param.sum()\n",
    "loss.backward()\n",
    "print(param.grad.dtype)  # torch.bfloat16\n",
    "\n",
    "# keep one FP32 master param\n",
    "master_param = param.detach().clone().float()  \n",
    "\n",
    "# cast grad to FP32\n",
    "grad_fp32 = param.grad.float()\n",
    "print(grad_fp32.dtype)\n",
    "\n",
    "# update master param with FP32 grad\n",
    "master_param -= 1e-3 * grad_fp32  \n",
    "\n",
    "# write back to model's bf16 param\n",
    "param.data.copy_(master_param.to(torch.bfloat16))\n",
    "param.data.dtype, param.grad.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a56266",
   "metadata": {},
   "source": [
    "**FSDP**\n",
    "\n",
    "FSDP 会把 参数 + 梯度 + 优化器状态 全部分片（shard），不同 GPU 只保留一份子集。\n",
    "- 参数存储\n",
    "    - Embedding (23M 参数)，如果 4 张卡：\n",
    "        - GPU0: 0 – 5.75M\n",
    "        - GPU1: 5.75M – 11.5M\n",
    "        - GPU2: 11.5M – 17.25M\n",
    "        - GPU3: 17.25M – 23M\n",
    "    - Transformer Block (40M 参数)，同理，每卡 10M；\n",
    "- 前向计算\n",
    "    - 当进入 Transformer Block 前，FSDP 会 all-gather 需要的参数（把 40M 的块临时拼回完整）；\n",
    "    - 也就是说，每张 GPU 拿到所有 shard 的副本，在 all-gather 结束后，每张卡都拥有完整的那一层权重；\n",
    "    - 因为每张卡都要独立完成前向和反向计算（各自算自己 batch 的 loss），数据都不一样；\n",
    "    - 为什么显存不会爆，这里的显存峰值是怎么控制住的？\n",
    "        - 按层 all-gather → 只在用到这一层时，才把 shard 聚合成完整参数；\n",
    "        - 用完马上释放（forward / backward 一结束就 free 掉）；\n",
    "- 反向计算\n",
    "    - 每张卡算自己的梯度，但只保留属于自己 shard 的那部分；\n",
    "    - FSDP 会做 reduce-scatter：把不同卡上的梯度分配到对应 shard，最后每张卡只保存自己负责的梯度；\n",
    "- 优化器状态\n",
    "    - Adam 优化器需要两个状态：m（动量）和 v（二阶矩估计）；\n",
    "    - 在 FSDP 下，m, v 也被切片，每张卡只保存 15.75M × 2；\n",
    "\n",
    "FSDP 显存节省 ≈ 4 倍。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
