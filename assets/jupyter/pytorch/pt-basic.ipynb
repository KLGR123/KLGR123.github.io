{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "241d53cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "196c089c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 5, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "conv1 = nn.Conv2d(1, 6, 5)\n",
    "conv2 = nn.Conv2d(6, 16, 5)\n",
    "conv_out = F.max_pool2d(conv2(F.max_pool2d(conv1(input), (2, 2))), 2)\n",
    "conv_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd0243af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 120])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "out1 = fc1(conv_out.flatten(1))\n",
    "out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c7dd49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc2 = nn.Linear(120, 10)\n",
    "out2 = F.relu(fc2(out1))\n",
    "out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc40d2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6889, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(out2, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6948dff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward0 object at 0x7f8d013158d0>\n",
      "<ReluBackward0 object at 0x7f8d01317cd0>\n",
      "<AddmmBackward0 object at 0x7f8d013142b0>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)\n",
    "print(loss.grad_fn.next_functions[0][0])\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c3becb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Linear(120, 50, dtype=torch.float64)\n",
    "\n",
    "    def foward(self, x):\n",
    "        return F.relu(self.fc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d96494f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 120]) torch.float64\n",
      "torch.Size([50]) torch.float64\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "for f in net.parameters():\n",
    "    print(f.data.shape, f.data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21dd9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b816814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bb6d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000,), -3.141592653589793, 3.141592653589793)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "x.shape, x[0], x[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2a9e257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000,), -1.2246467991473532e-16, -0.9999996912662218)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.sin(x)\n",
    "y.shape, y[0], y[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "565dd436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0574660160049302"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a564ce8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000,), 198434.87185622856)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "y_pred.shape, np.square(y_pred - y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67b9406b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999 8.817165410007025\n",
      "1999 8.817165410007025\n",
      "2999 8.817165410007025\n",
      "3999 8.817165410007025\n",
      "4999 8.817165410007025\n",
      "Result: y = 2.77e-17 + 0.857 x + -2.11e-17 x^2 + -0.0933 x^3\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-6\n",
    "for t in range(5000):\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "\n",
    "    if t % 1000 == 999:\n",
    "        print(t, loss)\n",
    "\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    a -= lr * grad_a\n",
    "    b -= lr * grad_b\n",
    "    c -= lr * grad_c\n",
    "    d -= lr * grad_d\n",
    "\n",
    "print(f'Result: y = {a:.3} + {b:.3} x + {c:.3} x^2 + {d:.3} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a9d5048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1., requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.full((), -1.0, device=torch.device(\"cpu\"), dtype=dtype, requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19b937e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 209.95834350585938\n",
      "199 144.66018676757812\n",
      "299 100.70249938964844\n",
      "399 71.03519439697266\n",
      "499 50.97850799560547\n",
      "599 37.403133392333984\n",
      "699 28.206867218017578\n",
      "799 21.973188400268555\n",
      "899 17.7457275390625\n",
      "999 14.877889633178711\n",
      "1099 12.93176555633545\n",
      "1199 11.610918045043945\n",
      "1299 10.714258193969727\n",
      "1399 10.10548210144043\n",
      "1499 9.692106246948242\n",
      "1599 9.411375999450684\n",
      "1699 9.220745086669922\n",
      "1799 9.091286659240723\n",
      "1899 9.003362655639648\n",
      "1999 8.943641662597656\n",
      "Result: y = -3.3247032904526463e-10 + -2.208526849746704 * P3(-2.395617870742939e-10 + 0.2554861009120941 x)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For this example, we need\n",
    "# 4 weights: y = a + b * P3(c + d * x), these weights need to be initialized\n",
    "# not too far from the correct result to ensure convergence.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 5e-6\n",
    "for t in range(2000):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'P3'.\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # P3 using our custom autograd operation.\n",
    "    y_pred = a + b * P3(c + d * x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9e72149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1805.1796875\n",
      "199 1207.095458984375\n",
      "299 808.5635375976562\n",
      "399 542.875244140625\n",
      "499 365.6612854003906\n",
      "599 247.39735412597656\n",
      "699 168.4302215576172\n",
      "799 115.67200469970703\n",
      "899 80.4023666381836\n",
      "999 56.809165954589844\n",
      "1099 41.01624298095703\n",
      "1199 30.437484741210938\n",
      "1299 23.346179962158203\n",
      "1399 18.589094161987305\n",
      "1499 15.395310401916504\n",
      "1599 13.249396324157715\n",
      "1699 11.806288719177246\n",
      "1799 10.835009574890137\n",
      "1899 10.180669784545898\n",
      "1999 9.73942756652832\n",
      "Result: y = 0.018791040405631065 + 0.8327891826629639 x + -0.003241765545681119 x^2 + -0.08992347121238708 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class Polynomial3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate four parameters and assign them as\n",
    "        member parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = Polynomial3()\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters (defined \n",
    "# with torch.nn.Parameter) which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n",
    "for t in range(2000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef61849d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3259), None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.nn.Parameter(torch.randn(()))\n",
    "a.data, a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a3965a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.randint(4, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4ccf1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 4138.115234375\n",
      "3999 1923.0311279296875\n",
      "5999 902.533203125\n",
      "7999 429.1354675292969\n",
      "9999 207.27027893066406\n",
      "11999 102.66796875\n",
      "13999 53.71826171875\n",
      "15999 29.76695442199707\n",
      "17999 19.074792861938477\n",
      "19999 13.861791610717773\n",
      "21999 11.00837516784668\n",
      "23999 9.752821922302246\n",
      "25999 9.414834976196289\n",
      "27999 9.126008987426758\n",
      "29999 8.98507308959961\n",
      "Result: y = 0.012064521200954914 + 0.8532969355583191 x + -0.0027639928739517927 x^2 + -0.09325013309717178 x^3 + 0.00013554545876104385 x^4 ? + 0.00013554545876104385 x^5 ?\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate five parameters and assign them as members.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "        self.e = torch.nn.Parameter(torch.randn(()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 4, 5\n",
    "        and reuse the e parameter to compute the contribution of these orders.\n",
    "\n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "\n",
    "        Here we also see that it is perfectly safe to reuse the same parameter many\n",
    "        times when defining a computational graph.\n",
    "        \"\"\"\n",
    "        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "        for exp in range(4, random.randint(4, 6)):\n",
    "            y = y + self.e * x ** exp\n",
    "        return y\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet()\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\n",
    "for t in range(30000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 2000 == 1999:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb4b9e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0126, -0.0498, -0.0186,  ..., -0.0418,  0.0047, -0.0299],\n",
       "        [ 0.0378,  0.0375,  0.0543,  ..., -0.0771, -0.0421,  0.0119],\n",
       "        [ 0.0012,  0.0659, -0.0649,  ..., -0.0226, -0.0375,  0.0742],\n",
       "        ...,\n",
       "        [ 0.0400, -0.0204, -0.0317,  ..., -0.0120, -0.0054,  0.0050],\n",
       "        [-0.0429, -0.0169,  0.0157,  ...,  0.0318,  0.0110, -0.0339],\n",
       "        [ 0.0092, -0.0273,  0.0347,  ...,  0.0091, -0.0041,  0.0499]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a624c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
