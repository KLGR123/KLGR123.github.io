{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dfed787",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "基于如上的 TRPO 的数学原理实现了 Minimal TRPO。\n",
    "\n",
    "三个数学工具核心\n",
    "- 共轭梯度 (Conjugate Gradient)\n",
    "- Hessian–向量积 (HVP) 基于 KL 的二阶近似\n",
    "- 线搜索 + KL 约束\n",
    "\n",
    "如下假定已经有一批 rollouts，并能提供\n",
    "```\n",
    "- states:      Tensor [N, obs_dim]\n",
    "- actions:     Tensor [N, act_dim]\n",
    "- advantages:  Tensor [N]\n",
    "- old_logp:    Tensor [N]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4920727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3661cf",
   "metadata": {},
   "source": [
    "如下是一个简单的高斯策略网络 (连续动作)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c432514",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim: int, act_dim: int, hidden_sizes=(64, 64)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = obs_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers += [nn.Linear(last, h), nn.Tanh()]\n",
    "            last = h\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.mu = nn.Linear(last, act_dim)\n",
    "        self.log_std = nn.Parameter(torch.zeros(act_dim))\n",
    "\n",
    "    def forward(self, obs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        h = self.net(obs)\n",
    "        mu = self.mu(h)\n",
    "        log_std = self.log_std.expand_as(mu)\n",
    "        return mu, log_std\n",
    "\n",
    "    def dist(self, obs: torch.Tensor) -> Normal:\n",
    "        mu, log_std = self(obs)\n",
    "        std = torch.exp(log_std)\n",
    "        return Normal(mu, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bbde3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_params(model: nn.Module) -> torch.Tensor:\n",
    "    return torch.cat([p.data.view(-1) for p in model.parameters()])\n",
    "\n",
    "\n",
    "def set_flat_params(model: nn.Module, flat: torch.Tensor) -> None:\n",
    "    i = 0\n",
    "    for p in model.parameters():\n",
    "        n = p.numel()\n",
    "        p.data.copy_(flat[i:i+n].view_as(p))\n",
    "        i += n\n",
    "\n",
    "\n",
    "def flat_grad(y: torch.Tensor, model: nn.Module, retain_graph=False, create_graph=False) -> torch.Tensor:\n",
    "    grads = torch.autograd.grad(y, [p for p in model.parameters() if p.requires_grad],\n",
    "                                retain_graph=retain_graph, create_graph=create_graph, allow_unused=True)\n",
    "    out = []\n",
    "    for g, p in zip(grads, model.parameters()):\n",
    "        if p.requires_grad:\n",
    "            out.append(torch.zeros_like(p).view(-1) if g is None else g.contiguous().view(-1))\n",
    "    return torch.cat(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea1b981",
   "metadata": {},
   "source": [
    "共轭梯度法 (只需提供 Avp 函数与右端项 b)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cde12840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradients(Avp: Callable[[torch.Tensor], torch.Tensor], b: torch.Tensor, nsteps=10, residual_tol=1e-10) -> torch.Tensor:\n",
    "    x = torch.zeros_like(b)\n",
    "    r = b.clone()\n",
    "    p = r.clone()\n",
    "    rr = torch.dot(r, r)\n",
    "    for _ in range(nsteps):\n",
    "        Ap = Avp(p)\n",
    "        alpha = rr / (torch.dot(p, Ap) + 1e-10)\n",
    "        x = x + alpha * p\n",
    "        r = r - alpha * Ap\n",
    "        new_rr = torch.dot(r, r)\n",
    "        if new_rr < residual_tol:\n",
    "            break\n",
    "        beta = new_rr / (rr + 1e-10)\n",
    "        p = r + beta * p\n",
    "        rr = new_rr\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d6626f",
   "metadata": {},
   "source": [
    "TRPO 更新器，包括可信域半径（阈值）、共轭梯度迭代步数、HVP 的阻尼项 (H + d * I)、线搜索收缩系数、线搜索最大步数、期望改进比例（可选用于更严格的接受准则）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d6c4228",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TRPOConfig:\n",
    "    max_kl: float = 1e-2             \n",
    "    cg_iters: int = 10                \n",
    "    cg_damping: float = 1e-2          \n",
    "    backtrack_coeff: float = 0.8      \n",
    "    backtrack_iters: int = 15         \n",
    "    accept_ratio: float = 0.1         \n",
    "\n",
    "\n",
    "class TRPOUpdater:\n",
    "    def __init__(self, policy: GaussianPolicy, config: TRPOConfig):\n",
    "        self.policy = policy\n",
    "        self.cfg = config\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _old_dist(self, states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        mu, log_std = self.policy(states)\n",
    "        return mu.detach(), log_std.detach()\n",
    "\n",
    "    def _surrogate_loss(self, states: torch.Tensor, actions: torch.Tensor, advantages: torch.Tensor, old_logp: torch.Tensor) -> torch.Tensor:\n",
    "        dist = self.policy.dist(states)\n",
    "        logp = dist.log_prob(actions).sum(-1)\n",
    "        ratio = torch.exp(logp - old_logp)\n",
    "        return -(ratio * advantages).mean()\n",
    "\n",
    "    def _mean_kl(self, states: torch.Tensor, old_mu: torch.Tensor, old_log_std: torch.Tensor) -> torch.Tensor:\n",
    "        new_mu, new_log_std = self.policy(states)\n",
    "        old_std, new_std = old_log_std.exp(), new_log_std.exp()\n",
    "        kl = (new_log_std - old_log_std) + (old_std.pow(2) + (old_mu - new_mu).pow(2)) / (2.0 * new_std.pow(2)) - 0.5\n",
    "        return kl.sum(dim=-1).mean()\n",
    "\n",
    "    def _build_hvp(self, states: torch.Tensor, old_mu: torch.Tensor, old_log_std: torch.Tensor) -> Callable[[torch.Tensor], torch.Tensor]:\n",
    "        def hvp(v: torch.Tensor) -> torch.Tensor:\n",
    "            kl = self._mean_kl(states, old_mu, old_log_std)\n",
    "            grads = flat_grad(kl, self.policy, create_graph=True)\n",
    "            gvp = (grads * v).sum()\n",
    "            hv = flat_grad(gvp, self.policy, retain_graph=True)\n",
    "            return hv + self.cfg.cg_damping * v\n",
    "        return hvp\n",
    "\n",
    "    def step(self, states: torch.Tensor, actions: torch.Tensor, advantages: torch.Tensor, old_logp: torch.Tensor) -> dict:\n",
    "        # keep track of old policy distribution\n",
    "        with torch.no_grad():\n",
    "            old_mu, old_log_std = self._old_dist(states)\n",
    "\n",
    "        # 1) compute policy gradient g\n",
    "        loss_pi = self._surrogate_loss(states, actions, advantages, old_logp)\n",
    "        g = flat_grad(loss_pi, self.policy)\n",
    "        g = -g  # maximize original objective => minimize negative objective, flip direction here\n",
    "\n",
    "        # if gradient is too small, return\n",
    "        if torch.norm(g) < 1e-8:\n",
    "            return {\"step_size\": 0.0, \"improve\": 0.0, \"kl\": 0.0}\n",
    "\n",
    "        # 2) solve Hx = g using conjugate gradient, get x ≈ H^{-1} g\n",
    "        hvp = self._build_hvp(states, old_mu, old_log_std)\n",
    "        x = conjugate_gradients(hvp, g, nsteps=self.cfg.cg_iters)\n",
    "\n",
    "        # 3) compute step size, ensure KL is less than max_kl theoretically\n",
    "        xHx = (x * hvp(x)).sum()\n",
    "        step_coef = math.sqrt(2.0 * self.cfg.max_kl / (xHx + 1e-10))\n",
    "        full_step = step_coef * x\n",
    "\n",
    "        # 4) line search, find step size that satisfies KL constraint and improves the objective\n",
    "        prev_params = flat_params(self.policy)\n",
    "        prev_loss = loss_pi.item()\n",
    "\n",
    "        def set_and_eval(step: torch.Tensor) -> Tuple[float, float]:\n",
    "            new_params = prev_params + step\n",
    "            set_flat_params(self.policy, new_params)\n",
    "            with torch.no_grad():\n",
    "                kl = self._mean_kl(states, old_mu, old_log_std).item()\n",
    "            loss_new = self._surrogate_loss(states, actions, advantages, old_logp).item()\n",
    "            improve = prev_loss - loss_new  # old loss - new loss, the larger the better\n",
    "            return improve, kl\n",
    "\n",
    "        step = full_step\n",
    "        for _ in range(self.cfg.backtrack_iters):\n",
    "            improve, kl = set_and_eval(step)\n",
    "            if kl <= self.cfg.max_kl and improve > 0:\n",
    "                break\n",
    "            step = self.cfg.backtrack_coeff * step\n",
    "        else:\n",
    "            # backtrack failed: restore parameters\n",
    "            set_flat_params(self.policy, prev_params)\n",
    "            return {\"step_size\": 0.0, \"improve\": 0.0, \"kl\": 0.0}\n",
    "\n",
    "        return {\"step_size\": float(step.norm().item()), \"improve\": float(improve), \"kl\": float(kl)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd99576",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "obs_dim, act_dim = 3, 2\n",
    "N = 256  # batch size\n",
    "\n",
    "policy = GaussianPolicy(obs_dim, act_dim)\n",
    "updater = TRPOUpdater(policy, TRPOConfig(max_kl=1e-2))\n",
    "\n",
    "states = torch.randn(N, obs_dim)\n",
    "actions = torch.randn(N, act_dim)\n",
    "advantages = torch.randn(N)\n",
    "\n",
    "with torch.no_grad():\n",
    "    dist_old = policy.dist(states)\n",
    "    old_logp = dist_old.log_prob(actions).sum(-1)\n",
    "\n",
    "info = updater.step(states, actions, advantages, old_logp)\n",
    "print(\"TRPO step info:\", info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
