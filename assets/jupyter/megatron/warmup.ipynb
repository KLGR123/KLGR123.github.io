{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5958e8d6",
   "metadata": {},
   "source": [
    "**Warmup**\n",
    "\n",
    "一开始网络还不稳定（权重随机、BN 统计未就绪、动量未热身），直接用大 LR 容易发散；warmup 先用小 LR 起步再平滑升到目标 LR。\n",
    "\n",
    "常见做法（$t$ 为第 t 步，$T_w$ 为 warmup 步数；$\\eta_{\\max}$ 为峰值 LR）：\n",
    "- Constant Warmup（常数热身）\n",
    "    - 前 $T_w$ 步用常数小 LR（如 $0.1\\eta_{\\max}$），之后切到主调度；\n",
    "    - 简单稳妥，适合对抖动敏感的任务；\n",
    "- Linear Warmup（线性热身，最常用）\n",
    "    - $\\eta(t) = \\eta_{\\max}\\cdot \\frac{t}{T_w}\\quad (t\\le T_w)$\n",
    "    - 线性爬坡到 $\\eta_{\\max}$，之后接主调度（余弦 / 多项式 / 恒定等）；\n",
    "- Exponential Warmup（指数热身）\n",
    "    - $\\eta(t) = \\eta_{\\max}\\cdot \\exp\\!\\big((t/T_w-1)\\cdot \\log r\\big)$；\n",
    "    - 早期更温和，适合极不稳定开局；\n",
    "- Cosine Warmup（余弦热身）\n",
    "    - $\\eta(t) = \\eta_{\\max}\\cdot \\frac{1-\\cos\\!\\big(\\pi\\cdot t/T_w\\big)}{2}$；\n",
    "\t- 与后续的余弦退火风格一致，整体更平滑；\n",
    "- Warmup + Inverse-Sqrt Decay（Transformer 常见）\n",
    "    $$\n",
    "    \\eta(t)=\\eta_{\\max}\\cdot\n",
    "    \\begin{cases}\n",
    "    \\frac{t}{T_w}, & t \\le T_w\\\\[2pt]\n",
    "    \\frac{\\sqrt{T_w}}{\\sqrt{t}}, & t>T_w\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    - 早期线性升，后期 $1/\\sqrt{t}$ 衰减；\n",
    "    - 适合序列建模、自注意力；\n",
    "- One-Cycle（单周期策略）\n",
    "    - 先热身到最大 LR，再用余弦降到很小的 LR；常配合反向动量曲线；\n",
    "    - 适合需要在固定步数里迅速到点的场景（如分类预训练）；\n",
    "\n",
    "如下是 Linear Warmup → Cosine Decay 的主流组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb3f3e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.nn.Linear(10, 10)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "total_steps = 10000\n",
    "warmup_steps = 800\n",
    "\n",
    "warmup = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer, start_factor=1e-3, end_factor=1.0, total_iters=warmup_steps\n",
    ")\n",
    "cosine = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=total_steps - warmup_steps, eta_min=1e-4\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "    optimizer, schedulers=[warmup, cosine], milestones=[warmup_steps]\n",
    ")\n",
    "\n",
    "for step in range(total_steps):\n",
    "    # ... forward/backward ...\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf31fb6",
   "metadata": {},
   "source": [
    "Warmup + Inverse-Sqrt（Transformer 风格）的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "064fd43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.nn.Linear(10, 10)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "total_steps  = 10000\n",
    "warmup_steps = 4000\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if step == 0:\n",
    "        return 1e-8  # avoid 0\n",
    "    if step <= warmup_steps:\n",
    "        return step / warmup_steps\n",
    "    else:\n",
    "        return (warmup_steps ** 0.5) / (step ** 0.5)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "for step in range(total_steps):\n",
    "    # ... forward/backward ...\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59d012c",
   "metadata": {},
   "source": [
    "One-Cycle 带 warmup 的单周期。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a207d2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.016]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.nn.Linear(10, 10)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "total_steps = 30000\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=0.4,                # peak LR after warmup\n",
    "    total_steps=total_steps,\n",
    "    pct_start=0.1,             # first 10% steps for warmup\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=25,             # initial LR = max_lr/div_factor\n",
    "    final_div_factor=1e4       # final LR = max_lr/final_div_factor\n",
    ")\n",
    "scheduler.base_lrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb75778",
   "metadata": {},
   "source": [
    "**LR ∝ B 线性缩放**\n",
    "\n",
    "- 一个 epoch 有 N/B 个 step；若把 batch 改成 kB，每个 epoch 的步数变成 N/(kB)，少了 k 倍；\n",
    "- 若把 LR 同时放大到 $k\\eta$，那么每个 epoch 的累计更新量大致不变 $(N/(kB))\\cdot k\\eta \\approx (N/B)\\cdot \\eta$；\n",
    "- 把 batch 放大 k 倍，就把 LR 也放大 k 倍，能维持接近的训练动力学（再配合 warmup 保证稳定）；\n",
    "\n",
    "或者说\n",
    "- 小批量梯度 $\\hat g=\\frac{1}{B}\\sum_{i=1}^B g_i$ 的方差 $\\text{Var}(\\hat g)\\propto 1/B$，batch 越大，噪声越小；\n",
    "- 当噪声减小时，可以承受更大的步长（LR）而不发散；线性缩放在很多视觉 / NLP大规模训练里被验证有效；\n",
    "\n",
    "但也不是无限线性，如果超过某个临界 batch，收益会变弱或泛化变差；这时候考虑增 warmup 步数、配 LARS / LAMB 等大 batch 友好优化器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ce785",
   "metadata": {},
   "source": [
    "**LARS（Layer-wise Adaptive Rate Scaling）**\n",
    "\n",
    "核心想法：对每一层算一个信任比率（trust ratio）\n",
    "$$\n",
    "\\text{trust\\_ratio}_\\ell\n",
    "\\;=\\;\n",
    "\\eta \\cdot \\frac{\\lVert w_\\ell\\rVert}{\\lVert g_\\ell \\rVert + \\epsilon}\n",
    "$$\n",
    "\n",
    "\n",
    "用它去缩放全局学习率\n",
    "\n",
    "$$\\text{lr}_\\ell = \\text{lr} \\times \\text{trust\\_ratio}_\\ell$$\n",
    "\n",
    "再做 SGD (+momentum) 更新。\n",
    "\n",
    "- 让每层的相对更新幅度 $\\lVert \\Delta w_\\ell\\rVert/\\lVert w_\\ell\\rVert$ 更一致，避免层间尺度差异在巨型 batch 下被放大导致训练不稳；\n",
    "- 常见用法是超大 batch 训练 CNN（如 ImageNet 32K + batch）；\n",
    "- 通常跳过 BN / 偏置层的层级自适应与权重衰减（trust ratio 置 1 即可）；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dfe7fe",
   "metadata": {},
   "source": [
    "**LAMB（Layer-wise Adaptive Moments for Batch training）**\n",
    "\n",
    "核心想法：先像 Adam / AdamW 一样算自适应步子（按二阶动量归一化），再加一层 layer-wise trust ratio；\n",
    "$$\n",
    "u_\\ell=\\frac{\\hat m_\\ell}{\\sqrt{\\hat v_\\ell}+\\epsilon} \\quad(\\text{Adam 方向})\n",
    "\\qquad\n",
    "r_\\ell=\\frac{\\lVert w_\\ell\\rVert}{\\lVert u_\\ell \\rVert+\\epsilon}\n",
    "\\qquad\n",
    "\\Delta w_\\ell=-\\text{lr}\\cdot r_\\ell\\cdot u_\\ell\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ffa9a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "def _exclude(p_name):\n",
    "    # If the name contains 'bn', 'ln', 'bias', do not adapt to layer-wise\n",
    "    n = p_name.lower()\n",
    "    return (\"bn\" in n) or (\"ln\" in n) or (\"bias\" in n)\n",
    "\n",
    "class LARS(Optimizer):\n",
    "    def __init__(self, params, lr=0.1, momentum=0.9, weight_decay=1e-4,\n",
    "                 eta=0.001, eps=1e-9, trust_clip=None):\n",
    "\n",
    "        # support (name, param)\n",
    "        named = []\n",
    "        raw = []\n",
    "        for p in params:\n",
    "            if isinstance(p, tuple):\n",
    "                named.append(p)\n",
    "            else:\n",
    "                raw.append(p)\n",
    "\n",
    "        if named:\n",
    "            param_groups = [{\"params\":[p for _, p in named], \"names\":[n for n, _ in named]}]\n",
    "        else:\n",
    "            param_groups = [{\"params\": raw, \"names\": [\"\" for _ in raw]}]\n",
    "\n",
    "        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay,\n",
    "                        eta=eta, eps=eps, trust_clip=trust_clip)\n",
    "        super().__init__(param_groups, defaults)\n",
    "\n",
    "        # allocate state for each param\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                self.state[p][\"momentum_buffer\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None if closure is None else closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]\n",
    "            mom = group[\"momentum\"]\n",
    "            wd  = group[\"weight_decay\"]\n",
    "            eta = group[\"eta\"]\n",
    "            eps = group[\"eps\"]\n",
    "            clip= group[\"trust_clip\"]\n",
    "            names = group.get(\"names\", [\"\"]*len(group[\"params\"]))\n",
    "\n",
    "            for name, p in zip(names, group[\"params\"]):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                g = p.grad\n",
    "\n",
    "                # decoupled weight decay\n",
    "                if wd != 0 and not _exclude(name):\n",
    "                    p.add_(p, alpha=-lr*wd)\n",
    "\n",
    "                # momentum\n",
    "                buf = self.state[p][\"momentum_buffer\"]\n",
    "                buf.mul_(mom).add_(g)\n",
    "\n",
    "                # layer-wise trust ratio (not adapt to BN/LN/bias)\n",
    "                if _exclude(name):\n",
    "                    trust_ratio = 1.0\n",
    "                else:\n",
    "                    w_norm = p.norm()\n",
    "                    u_norm = buf.norm()\n",
    "                    if w_norm > 0 and u_norm > 0:\n",
    "                        trust_ratio = eta * (w_norm / (u_norm + eps))\n",
    "                        if clip is not None:\n",
    "                            trust_ratio = min(trust_ratio, clip)\n",
    "                    else:\n",
    "                        trust_ratio = 1.0\n",
    "\n",
    "                # update\n",
    "                p.add_(buf, alpha=-lr * trust_ratio)\n",
    "\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
