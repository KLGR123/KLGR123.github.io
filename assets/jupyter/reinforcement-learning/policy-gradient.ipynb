{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7347f8eb",
   "metadata": {},
   "source": [
    "策略梯度算法一切都是那么的第一性和直接，在目标函数构造上也与带 soft label 的 CE loss 相呼应，它们都在通过 KL 散度靠拢离线数据（在线环境与奖励）与模型 $\\theta$ 之间的分布。构造 log 那一步再最开始我只理解为数学技巧，但现在看山仍是山，这一数学技巧将期望结构拽了出来。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\\nabla \\bar{R}_{\\theta} & =\\sum_{\\tau} R(\\tau) \\nabla p_{\\theta}(\\tau) \\\\& =\\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau) \\frac{\\nabla p_{\\theta}(\\tau)}{p_{\\theta}(\\tau)} \\\\& =\\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau) \\nabla \\log p_{\\theta}(\\tau) \\\\& =\\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}\\left[R(\\tau) \\nabla \\log p_{\\theta}(\\tau)\\right] \\\\& \\approx \\frac{1}{N} \\sum_{n=1}^{N} R\\left(\\tau^{n}\\right) \\nabla \\log p_{\\theta}\\left(\\tau^{n}\\right) \\\\& =\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} R\\left(\\tau^{n}\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right)\\end{aligned}\n",
    "$$\n",
    "\n",
    "用梯度上升来更新参数。\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta+\\eta \\nabla \\bar{R}_{\\theta}\n",
    "$$\n",
    "\n",
    "两个技巧，一个是加上 baseline，实际上就是优势函数；另一个是奖励函数分配，实际上就是将整体轨迹的奖励按时间粒度替换成 t 时刻的回报 $G_t$。\n",
    "\n",
    "策略梯度方法\n",
    "- 如果基于 MC 蒙特卡洛去估计优势函数，则是 REINFORCE 算法，即采样一条轨迹之后，循环维护 G 回报计算每个时间点的优势；\n",
    "- 如果基于 TD 时序差分算法，即换成 $Q(s_t, a_t)$ 函数估计每一时刻的未来回报（或优势），则是 AC 算法；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d834b3d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65061554",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
