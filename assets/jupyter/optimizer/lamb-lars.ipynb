{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e7390dd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**LARS（Layer-wise Adaptive Rate Scaling）**\n",
    "\n",
    "核心想法：对每一层算一个信任比率（trust ratio）\n",
    "$$\n",
    "\\text{trust\\_ratio}_\\ell\n",
    "\\;=\\;\n",
    "\\eta \\cdot \\frac{\\lVert w_\\ell\\rVert}{\\lVert g_\\ell \\rVert + \\epsilon}\n",
    "$$\n",
    "\n",
    "\n",
    "用它去缩放全局学习率\n",
    "\n",
    "$$\\text{lr}_\\ell = \\text{lr} \\times \\text{trust\\_ratio}_\\ell$$\n",
    "\n",
    "再做 SGD (+momentum) 更新。\n",
    "\n",
    "- 让每层的相对更新幅度 $\\lVert \\Delta w_\\ell\\rVert/\\lVert w_\\ell\\rVert$ 更一致，避免层间尺度差异在巨型 batch 下被放大导致训练不稳；\n",
    "- 常见用法是超大 batch 训练 CNN（如 ImageNet 32K + batch）；\n",
    "- 通常跳过 BN / 偏置层的层级自适应与权重衰减（trust ratio 置 1 即可）；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a60679",
   "metadata": {},
   "source": [
    "**LAMB（Layer-wise Adaptive Moments for Batch training）**\n",
    "\n",
    "核心想法：先像 Adam / AdamW 一样算自适应步子（按二阶动量归一化），再加一层 layer-wise trust ratio；\n",
    "$$\n",
    "u_\\ell=\\frac{\\hat m_\\ell}{\\sqrt{\\hat v_\\ell}+\\epsilon} \\quad(\\text{Adam 方向})\n",
    "\\qquad\n",
    "r_\\ell=\\frac{\\lVert w_\\ell\\rVert}{\\lVert u_\\ell \\rVert+\\epsilon}\n",
    "\\qquad\n",
    "\\Delta w_\\ell=-\\text{lr}\\cdot r_\\ell\\cdot u_\\ell\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2e6907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "def _exclude(p_name):\n",
    "    # If the name contains 'bn', 'ln', 'bias', do not adapt to layer-wise\n",
    "    n = p_name.lower()\n",
    "    return (\"bn\" in n) or (\"ln\" in n) or (\"bias\" in n)\n",
    "\n",
    "class LARS(Optimizer):\n",
    "    def __init__(self, params, lr=0.1, momentum=0.9, weight_decay=1e-4,\n",
    "                 eta=0.001, eps=1e-9, trust_clip=None):\n",
    "\n",
    "        # support (name, param)\n",
    "        named = []\n",
    "        raw = []\n",
    "        for p in params:\n",
    "            if isinstance(p, tuple):\n",
    "                named.append(p)\n",
    "            else:\n",
    "                raw.append(p)\n",
    "\n",
    "        if named:\n",
    "            param_groups = [{\"params\":[p for _, p in named], \"names\":[n for n, _ in named]}]\n",
    "        else:\n",
    "            param_groups = [{\"params\": raw, \"names\": [\"\" for _ in raw]}]\n",
    "\n",
    "        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay,\n",
    "                        eta=eta, eps=eps, trust_clip=trust_clip)\n",
    "        super().__init__(param_groups, defaults)\n",
    "\n",
    "        # allocate state for each param\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                self.state[p][\"momentum_buffer\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None if closure is None else closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]\n",
    "            mom = group[\"momentum\"]\n",
    "            wd  = group[\"weight_decay\"]\n",
    "            eta = group[\"eta\"]\n",
    "            eps = group[\"eps\"]\n",
    "            clip= group[\"trust_clip\"]\n",
    "            names = group.get(\"names\", [\"\"]*len(group[\"params\"]))\n",
    "\n",
    "            for name, p in zip(names, group[\"params\"]):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                g = p.grad\n",
    "\n",
    "                # decoupled weight decay\n",
    "                if wd != 0 and not _exclude(name):\n",
    "                    p.add_(p, alpha=-lr*wd)\n",
    "\n",
    "                # momentum\n",
    "                buf = self.state[p][\"momentum_buffer\"]\n",
    "                buf.mul_(mom).add_(g)\n",
    "\n",
    "                # layer-wise trust ratio (not adapt to BN/LN/bias)\n",
    "                if _exclude(name):\n",
    "                    trust_ratio = 1.0\n",
    "                else:\n",
    "                    w_norm = p.norm()\n",
    "                    u_norm = buf.norm()\n",
    "                    if w_norm > 0 and u_norm > 0:\n",
    "                        trust_ratio = eta * (w_norm / (u_norm + eps))\n",
    "                        if clip is not None:\n",
    "                            trust_ratio = min(trust_ratio, clip)\n",
    "                    else:\n",
    "                        trust_ratio = 1.0\n",
    "\n",
    "                # update\n",
    "                p.add_(buf, alpha=-lr * trust_ratio)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab12890a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
