{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86221031",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50e88f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    A drop-in Linear with LoRA adaptation.\n",
    "    - base: a frozen big Linear (weight/bias not trainable)\n",
    "    - lora_A: (r, in_features)\n",
    "    - lora_B: (out_features, r)\n",
    "    forward: x @ W^T + (alpha/r) * x @ A^T @ B^T + b\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        r: int = 8,\n",
    "        alpha: float = 16.0,\n",
    "        dropout: float = 0.1,\n",
    "        bias: bool = True,\n",
    "        init_scale: float = 1e-4, # small init for A, zeros for B (or vice-versa)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert r > 0, \"LoRA rank r must be > 0\"\n",
    "\n",
    "        self.base = nn.Linear(in_features, out_features, bias=bias)\n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = False # frozen\n",
    "\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "        self.lora_A = nn.Parameter(torch.empty(r, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.empty(out_features, r))\n",
    "        # A with small init, B with zeros init\n",
    "        nn.init.normal_(self.lora_A, std=init_scale)\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.merged = False # add BA to base.weight when inference\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.merged:\n",
    "            return self.base(x)\n",
    "\n",
    "        # forward + LoRA path\n",
    "        base_out = self.base(x)\n",
    "        lora_out = F.linear(self.dropout(x), self.lora_A)           # x @ A^T\n",
    "        lora_out = F.linear(lora_out, self.lora_B) * self.scaling   # (xA^T) @ B^T\n",
    "        return base_out + lora_out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def merge_weights_(self):\n",
    "        \"\"\"add BA to base.weight when inference\"\"\"\n",
    "        if self.merged:\n",
    "            return\n",
    "        delta_w = (self.lora_B @ self.lora_A) * self.scaling  # [out, in]\n",
    "        self.base.weight += delta_w\n",
    "        self.merged = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5be60e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 16])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TinyAdapter(nn.Module):\n",
    "    \"\"\"\n",
    "    two-layer MLP, hidden layer is frozen base Linear,\n",
    "    second layer is LoRA for task adaptation.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in=128, d_hidden=256, d_out=16, r=8, alpha=16.0):\n",
    "        super().__init__()\n",
    "        # backbone: frozen\n",
    "        self.backbone = nn.Linear(d_in, d_hidden, bias=True)\n",
    "        # LoRA: trainable\n",
    "        self.head = LoRALinear(d_hidden, d_out, r=r, alpha=alpha, bias=True)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.backbone(x))\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "adapter = TinyAdapter()\n",
    "x = torch.randn(10, 128)\n",
    "adapter(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c17c19b",
   "metadata": {},
   "source": [
    "这里的数据构造相对巧妙，通过两个略有区别的线性映射构造标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d53bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4096, 16]), torch.Size([512, 16]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_task_matrices(d_in=128, d_hidden=256, d_out=16, device=\"cpu\"):\n",
    "    torch.manual_seed(0)\n",
    "    W_old1 = torch.randn(d_hidden, d_in) / math.sqrt(d_in)\n",
    "    b_old1 = torch.randn(d_hidden) * 0.1\n",
    "    W_old2 = torch.randn(d_out, d_hidden) / math.sqrt(d_hidden)\n",
    "    b_old2 = torch.randn(d_out) * 0.1\n",
    "    shift = 0.15\n",
    "    W_new2 = W_old2 + shift * torch.randn_like(W_old2)\n",
    "    b_new2 = b_old2 + shift * torch.randn_like(b_old2)\n",
    "    mats = dict(\n",
    "        W_old1=W_old1.to(device), b_old1=b_old1.to(device),\n",
    "        W_old2=W_old2.to(device), b_old2=b_old2.to(device),\n",
    "        W_new2=W_new2.to(device), b_new2=b_new2.to(device),\n",
    "    )\n",
    "    return mats\n",
    "\n",
    "def synth_data(n=4096, d_in=128, device=\"cpu\"):\n",
    "    x = torch.randn(n, d_in, device=device)\n",
    "    return x\n",
    "\n",
    "def forward_with_mats(x, W1, b1, W2, b2):\n",
    "    h = F.gelu(F.linear(x, W1, b1))\n",
    "    y = F.linear(h, W2, b2)\n",
    "    return y\n",
    "\n",
    "device = \"mps\" if torch.cuda.is_available() else \"cpu\"\n",
    "d_in, d_hidden, d_out = 128, 256, 16\n",
    "\n",
    "mats = make_task_matrices(d_in, d_hidden, d_out, device=device)\n",
    "x_train = synth_data(4096, d_in, device=device)\n",
    "x_val = synth_data(512, d_in, device=device)\n",
    "\n",
    "# target labels: from old/new task\n",
    "y_old_train = forward_with_mats(\n",
    "    x_train, mats[\"W_old1\"], mats[\"b_old1\"], mats[\"W_old2\"], mats[\"b_old2\"]\n",
    ")\n",
    "y_new_train = forward_with_mats(\n",
    "    x_train, mats[\"W_old1\"], mats[\"b_old1\"], mats[\"W_new2\"], mats[\"b_new2\"]\n",
    ")\n",
    "y_new_val = forward_with_mats(\n",
    "    x_val, mats[\"W_old1\"], mats[\"b_old1\"], mats[\"W_new2\"], mats[\"b_new2\"]\n",
    ")\n",
    "\n",
    "y_new_train.shape, y_new_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b38904",
   "metadata": {},
   "source": [
    "如下是预训练和后训练的简单模拟，核心在于\n",
    "- 通过 requires_grad 指定是否需要梯度；\n",
    "- 为 AdamW 优化器提供 lambda 表达式以指定哪些参数是需要梯度更新的；\n",
    "- 最后，merged 后 base.weight += delta_w 即可；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d733144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pretrain] step 100, loss=0.0661\n",
      "[Pretrain] step 200, loss=0.0438\n",
      "[Pretrain] step 300, loss=0.0338\n",
      "[Pretrain] step 400, loss=0.0285\n",
      "[Pretrain] step 500, loss=0.0251\n",
      "[Pretrain] step 600, loss=0.0228\n",
      "[Pretrain] step 700, loss=0.0211\n",
      "[Pretrain] step 800, loss=0.0198\n",
      "\n",
      "Trainable params after freezing: 2192/39312 (5.58%). only LoRA is trainable.\n",
      "[LoRA finetune] step 100, train_loss=1.3054, val_loss=1.4206\n",
      "[LoRA finetune] step 200, train_loss=1.2314, val_loss=1.3741\n",
      "[LoRA finetune] step 300, train_loss=1.2226, val_loss=1.3675\n",
      "[LoRA finetune] step 400, train_loss=1.2155, val_loss=1.3663\n",
      "[LoRA finetune] step 500, train_loss=1.2152, val_loss=1.3567\n",
      "[LoRA finetune] step 600, train_loss=1.2190, val_loss=1.3624\n",
      "\n",
      "Merged for inference. Val loss (merged): 1.2944\n"
     ]
    }
   ],
   "source": [
    "r, alpha = 8, 16.0\n",
    "model = TinyAdapter(d_in, d_hidden, d_out, r=r, alpha=alpha).to(device)\n",
    "\n",
    "# pretrain\n",
    "for p in model.backbone.parameters():\n",
    "    p.requires_grad = True\n",
    "for p in model.head.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "opt = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for step in range(800):\n",
    "    opt.zero_grad()\n",
    "    pred = model(x_train)\n",
    "    loss = loss_fn(pred, y_old_train)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if (step+1) % 100 == 0:\n",
    "        print(f\"[Pretrain] step {step+1}, loss={loss.item():.4f}\")\n",
    "\n",
    "# finetune\n",
    "for p in model.backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "for n, p in model.head.named_parameters():\n",
    "    p.requires_grad = (\"lora_\" in n) or (\"bias\" in n)  # train LoRA A/B and optional bias\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTrainable params after freezing: {trainable}/{total} ({100*trainable/total:.2f}%). only LoRA is trainable.\")\n",
    "\n",
    "opt = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-3)\n",
    "\n",
    "for step in range(600):\n",
    "    opt.zero_grad()\n",
    "    pred = model(x_train)\n",
    "    loss = loss_fn(pred, y_new_train)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if (step+1) % 100 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(x_val)\n",
    "            val_loss = loss_fn(val_pred, y_new_val).item()\n",
    "        print(f\"[LoRA finetune] step {step+1}, train_loss={loss.item():.4f}, val_loss={val_loss:.4f}\")\n",
    "\n",
    "# merge weights\n",
    "model.head.merge_weights_()  # after forward, no need to use A/B\n",
    "with torch.no_grad():\n",
    "    val_pred = model(x_val)\n",
    "    val_loss = loss_fn(val_pred, y_new_val).item()\n",
    "print(f\"\\nMerged for inference. Val loss (merged): {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7ace63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
